{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "relevant-folder",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-24T07:56:15.951593Z",
     "start_time": "2021-03-24T07:56:15.934589Z"
    }
   },
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "athletic-outside",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-24T07:56:16.564619Z",
     "start_time": "2021-03-24T07:56:16.552456Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "trying-poverty",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-24T07:56:16.789156Z",
     "start_time": "2021-03-24T07:56:16.781715Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from pprint import pprint\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "active-trigger",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-24T07:56:18.558716Z",
     "start_time": "2021-03-24T07:56:16.982551Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import itertools\n",
    "from time import time\n",
    "from collections import defaultdict\n",
    "\n",
    "import regex\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import editdistance\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from ctcdecode import CTCBeamDecoder\n",
    "\n",
    "import torchaudio\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, BatchSampler, SequentialSampler, RandomSampler\n",
    "\n",
    "from vocabulary import Vocab\n",
    "\n",
    "from src.audio_utils import open_audio\n",
    "from src.audio_utils import make_transform, get_default_audio_transforms\n",
    "from src.audio_utils import AudioTransformsChain, AudioTransformsExclusive\n",
    "from src.audio_utils import SpectrogramTransform, compute_log_mel_spectrogram\n",
    "\n",
    "from src.datasets import AudioDataset\n",
    "from src.datasets import AudioDatasetSampler, collate_fn\n",
    "\n",
    "from src.datasets import manifest_train_test_split\n",
    "from src.datasets import convert_libri_manifest_to_common_voice\n",
    "from src.datasets import convert_open_stt_manifest_to_common_voice\n",
    "\n",
    "from src.deepspeech import Model\n",
    "\n",
    "from src.decoding import calc_wer, calc_wer_for_batch\n",
    "from src.decoding import decode, greedy_decoder, beam_search_decode, fast_beam_search_decode\n",
    "\n",
    "from src.optimization import get_prediction, get_model_results\n",
    "from src.optimization import get_prediction, get_model_results, validate, training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "continued-inspection",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-24T07:56:18.661274Z",
     "start_time": "2021-03-24T07:56:18.560919Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtype: torch.float32, device: cuda:0, cuda_device_id 0\n"
     ]
    }
   ],
   "source": [
    "# Set proper device for computations,\n",
    "dtype, device, cuda_device_id = torch.float32, None, 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '{0}'.format(str(cuda_device_id) if cuda_device_id is not None else '')\n",
    "if cuda_device_id is not None and torch.cuda.is_available():\n",
    "    device = 'cuda:{0:d}'.format(0)\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "print(f'dtype: {dtype}, device: {device}, cuda_device_id {cuda_device_id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "likely-lawyer",
   "metadata": {},
   "source": [
    "# Create Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "tired-lindsay",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-24T07:56:19.881702Z",
     "start_time": "2021-03-24T07:56:19.862793Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_num_tokens(vocab):\n",
    "    ### write your code here ###\n",
    "    num_tokens = len(vocab.tokens2indices())\n",
    "    return num_tokens\n",
    "\n",
    "def get_blank_index(vocab):\n",
    "    ### write your code here ###\n",
    "    blank_index = vocab['<blank>']\n",
    "    return blank_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "incoming-alliance",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-24T07:56:20.005808Z",
     "start_time": "2021-03-24T07:56:19.988605Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The `unk_token` '<unk>' wasn't found in the tokens. Adding the `unk_token` to the end of the Vocab.\n"
     ]
    }
   ],
   "source": [
    "alphabet = [\n",
    "    'а', 'б', 'в', 'г', 'д', 'е', 'ё', 'ж', 'з', 'и', 'й', 'к',\n",
    "    'л', 'м', 'н', 'о', 'п', 'р', 'с', 'т', 'у', 'ф', 'х', 'ц',\n",
    "    'ч', 'ш', 'щ', 'ь', 'ы', 'ъ', 'э', 'ю', 'я', ' ', '<blank>'\n",
    "]\n",
    "\n",
    "vocab = Vocab(alphabet)\n",
    "\n",
    "num_tokens = get_num_tokens(vocab) \n",
    "blank_index = get_blank_index(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interracial-hacker",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T14:16:50.445235Z",
     "start_time": "2021-03-23T14:16:50.427831Z"
    }
   },
   "source": [
    "# Choose Audio Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "disabled-missouri",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-24T07:56:21.140716Z",
     "start_time": "2021-03-24T07:56:21.125442Z"
    }
   },
   "outputs": [],
   "source": [
    "# audio_transforms = get_default_audio_transforms()\n",
    "audio_transforms = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "encouraging-symbol",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-24T07:56:21.311388Z",
     "start_time": "2021-03-24T07:56:21.296511Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_rate = 8000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitted-walker",
   "metadata": {},
   "source": [
    "# Create Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seeing-generator",
   "metadata": {},
   "source": [
    "## Load Common Voice dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "knowing-wireless",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-24T07:56:22.971027Z",
     "start_time": "2021-03-24T07:56:22.752999Z"
    }
   },
   "outputs": [],
   "source": [
    "common_voice_val_manifest_path = '/home/e.chuykova/data/val.txt'\n",
    "common_voice_test_manifest_path = '/home/e.chuykova/data/test.txt'\n",
    "common_voice_train_manifest_path = '/home/e.chuykova/data/train.txt'\n",
    "\n",
    "common_voice_val_dataset = AudioDataset(\n",
    "    common_voice_val_manifest_path, vocab, sample_rate=sample_rate,\n",
    "#     evaluate_stats=True\n",
    ")\n",
    "common_voice_test_dataset = AudioDataset(\n",
    "    common_voice_test_manifest_path, vocab, sample_rate=sample_rate,\n",
    "#     evaluate_stats=True\n",
    ")\n",
    "common_voice_train_dataset = AudioDataset(\n",
    "    common_voice_train_manifest_path, vocab, sample_rate=sample_rate,\n",
    "    audio_transforms=audio_transforms,\n",
    "#     evaluate_stats=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formal-consultation",
   "metadata": {},
   "source": [
    "## Load LibriSpeech dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "found-bathroom",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-24T07:56:25.598829Z",
     "start_time": "2021-03-24T07:56:24.522919Z"
    }
   },
   "outputs": [],
   "source": [
    "ls_dev_manifest_path = '/data/mnakhodnov/voice_data/libri_speech/dev/manifest.json'\n",
    "ls_test_manifest_path = '/data/mnakhodnov/voice_data/libri_speech/test/manifest.json'\n",
    "ls_train_manifest_path = '/data/mnakhodnov/voice_data/libri_speech/train/manifest.json'\n",
    "\n",
    "ls_dev_manifest_path = convert_libri_manifest_to_common_voice(ls_dev_manifest_path)\n",
    "ls_test_manifest_path = convert_libri_manifest_to_common_voice(ls_test_manifest_path)\n",
    "ls_train_manifest_path = convert_libri_manifest_to_common_voice(ls_train_manifest_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "artificial-palmer",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-24T07:56:26.801459Z",
     "start_time": "2021-03-24T07:56:26.554264Z"
    }
   },
   "outputs": [],
   "source": [
    "ls_dev_dataset = AudioDataset(\n",
    "    ls_dev_manifest_path, vocab=vocab, sample_rate=sample_rate, max_duration=10.0,\n",
    "#     evaluate_stats=True\n",
    ")\n",
    "ls_test_dataset = AudioDataset(\n",
    "    ls_test_manifest_path, vocab=vocab, sample_rate=sample_rate, max_duration=10.0,\n",
    "#     evaluate_stats=True\n",
    ")\n",
    "ls_train_dataset = AudioDataset(\n",
    "    ls_train_manifest_path, vocab=vocab, sample_rate=sample_rate, max_duration=10.0,\n",
    "    audio_transforms=audio_transforms,\n",
    "#     evaluate_stats=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checked-russell",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-22T11:00:14.720362Z",
     "start_time": "2021-03-22T11:00:14.703290Z"
    }
   },
   "source": [
    "## Load Open STT (radio_2) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "amazing-contributor",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-24T07:56:43.735624Z",
     "start_time": "2021-03-24T07:56:43.713941Z"
    }
   },
   "outputs": [],
   "source": [
    "open_stt_manifest_path = '/data/mnakhodnov/voice_data/radio_2/radio_2.csv'\n",
    "\n",
    "# open_stt_manifest_path = convert_open_stt_manifest_to_common_voice(open_stt_manifest_path, min_duration=2.0)\n",
    "# open_stt_test_manifest_path, open_stt_train_manifest_path = manifest_train_test_split(open_stt_manifest_path, ratio=0.005)\n",
    "\n",
    "open_stt_manifest_path = '/data/mnakhodnov/voice_data/radio_2/radio_2.common_voice.csv'\n",
    "open_stt_test_manifest_path = '/data/mnakhodnov/voice_data/radio_2/radio_2.common_voice_test.csv'\n",
    "open_stt_train_manifest_path = '/data/mnakhodnov/voice_data/radio_2/radio_2.common_voice_train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "asian-developer",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-24T07:56:48.353302Z",
     "start_time": "2021-03-24T07:56:44.769451Z"
    }
   },
   "outputs": [],
   "source": [
    "open_stt_test_dataset = AudioDataset(\n",
    "    open_stt_test_manifest_path, vocab=vocab, sample_rate=sample_rate, min_duration=2.0, max_duration=10.0,\n",
    "#     evaluate_stats=True\n",
    ")\n",
    "open_stt_train_dataset = AudioDataset(\n",
    "    open_stt_train_manifest_path, vocab=vocab, sample_rate=sample_rate, min_duration=2.0, max_duration=10.0,\n",
    "    audio_transforms=audio_transforms,\n",
    "#     evaluate_stats=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "primary-force",
   "metadata": {},
   "source": [
    "## Combine all datasets for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cellular-western",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-24T07:56:52.971910Z",
     "start_time": "2021-03-24T07:56:49.563225Z"
    }
   },
   "outputs": [],
   "source": [
    "combined_dataset = AudioDataset(\n",
    "    [common_voice_train_manifest_path, ls_train_manifest_path, open_stt_train_manifest_path], \n",
    "    vocab=vocab, sample_rate=sample_rate, max_duration=10.0,\n",
    "    audio_transforms=audio_transforms,\n",
    "#     evaluate_stats=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latest-merit",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T14:19:15.279798Z",
     "start_time": "2021-03-23T14:19:15.262876Z"
    }
   },
   "source": [
    "# Create Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dirty-causing",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-24T07:56:53.620595Z",
     "start_time": "2021-03-24T07:56:53.598981Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 80\n",
    "num_workers = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rational-orange",
   "metadata": {},
   "source": [
    "## Common Voice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "charitable-behalf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-24T07:56:58.134390Z",
     "start_time": "2021-03-24T07:56:58.100722Z"
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "common_voice_val_dataloader = DataLoader(\n",
    "    common_voice_val_dataset, batch_size=batch_size, shuffle=False, \n",
    "    num_workers=num_workers, pin_memory=True, collate_fn=collate_fn\n",
    ")\n",
    "# YOUR CODE\n",
    "common_voice_test_dataloader = DataLoader(\n",
    "    common_voice_test_dataset, batch_size=batch_size, shuffle=False, \n",
    "    num_workers=num_workers, pin_memory=True, collate_fn=collate_fn\n",
    ")\n",
    "# YOUR CODE\n",
    "common_voice_train_dataloader = DataLoader(\n",
    "    common_voice_train_dataset, batch_size=batch_size, \n",
    "    sampler=AudioDatasetSampler(common_voice_train_dataset, batch_size=batch_size),\n",
    "    num_workers=num_workers, pin_memory=True, collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesbian-proportion",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T14:19:15.371150Z",
     "start_time": "2021-03-23T14:19:15.356303Z"
    }
   },
   "source": [
    "## Libri Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "confused-advertising",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-24T07:56:59.418456Z",
     "start_time": "2021-03-24T07:56:59.384077Z"
    }
   },
   "outputs": [],
   "source": [
    "ls_dev_dataloader = DataLoader(\n",
    "    ls_dev_dataset, batch_size=batch_size, shuffle=False, \n",
    "    num_workers=num_workers, pin_memory=True, collate_fn=collate_fn\n",
    ")\n",
    "ls_test_dataloader = DataLoader(\n",
    "    ls_test_dataset, batch_size=batch_size, shuffle=False, \n",
    "    num_workers=num_workers, pin_memory=True, collate_fn=collate_fn\n",
    ")\n",
    "ls_train_dataloader = DataLoader(\n",
    "    ls_train_dataset, batch_size=batch_size, \n",
    "    sampler=AudioDatasetSampler(ls_train_dataset, batch_size=batch_size),\n",
    "    num_workers=num_workers, pin_memory=True, collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriental-professor",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T14:19:45.209964Z",
     "start_time": "2021-03-23T14:19:45.192687Z"
    }
   },
   "source": [
    "## OpenSTT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "surprising-messenger",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-24T07:57:00.506087Z",
     "start_time": "2021-03-24T07:57:00.414772Z"
    }
   },
   "outputs": [],
   "source": [
    "open_stt_test_dataloader = DataLoader(\n",
    "    open_stt_test_dataset, batch_size=batch_size, shuffle=False, \n",
    "    num_workers=num_workers, pin_memory=True, collate_fn=collate_fn\n",
    ")\n",
    "open_stt_train_dataloader = DataLoader(\n",
    "    open_stt_train_dataset, batch_size=batch_size, \n",
    "    sampler=AudioDatasetSampler(open_stt_train_dataset, batch_size=batch_size),\n",
    "    num_workers=num_workers, pin_memory=True, collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intellectual-charter",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T15:04:47.713051Z",
     "start_time": "2021-03-23T15:04:47.695886Z"
    }
   },
   "source": [
    "## Combined Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "impaired-triple",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-24T07:57:01.620627Z",
     "start_time": "2021-03-24T07:57:01.498997Z"
    }
   },
   "outputs": [],
   "source": [
    "combined_dataloader = DataLoader(\n",
    "    combined_dataset, batch_size=batch_size, \n",
    "    sampler=AudioDatasetSampler(combined_dataset, batch_size=batch_size),\n",
    "    num_workers=num_workers, pin_memory=True, collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fantastic-findings",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "catholic-thumbnail",
   "metadata": {},
   "source": [
    "## Choose LM for beam search decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "friendly-whale",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-24T07:57:02.421583Z",
     "start_time": "2021-03-24T07:57:02.401814Z"
    }
   },
   "outputs": [],
   "source": [
    "kenlm_bin_path = '/home/mnakhodnov/kenlm/build/bin'\n",
    "\n",
    "# This models are sorted wr to their size and speed \n",
    "# kenlm_data_path = '/data/mnakhodnov/language_data/cc100/xaa.processed.1'\n",
    "# kenlm_data_path = '/data/mnakhodnov/language_data/cc100/xaa.processed.2'\n",
    "# kenlm_data_path = '/data/mnakhodnov/language_data/cc100/xaa.processed.3'\n",
    "# kenlm_data_path = '/data/mnakhodnov/language_data/cc100/xaa.processed.4'\n",
    "kenlm_data_path = '/data/mnakhodnov/language_data/common_voice/train.txt'\n",
    "kenlm_arpa_path, kenlm_binary_path = kenlm_data_path + '.arpa', kenlm_data_path + '.binary'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "purple-sympathy",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-24T07:57:03.445195Z",
     "start_time": "2021-03-24T07:57:03.430109Z"
    }
   },
   "outputs": [],
   "source": [
    "fast_beam_kwargs = {\n",
    "    'beam_size': 10, 'cutoff_top_n': 5, 'cutoff_prob': 1.0, \n",
    "    'ext_scoring_func': kenlm_binary_path, 'alpha': 1.0, 'beta': 0.3, 'num_processes': 32\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "handmade-agenda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-24T07:57:04.369778Z",
     "start_time": "2021-03-24T07:57:04.350435Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_from_ckpt(model, ckpt_path):\n",
    "    checkpoint = torch.load(ckpt_path, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "requested-norway",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-24T07:57:06.737719Z",
     "start_time": "2021-03-24T07:57:06.715198Z"
    }
   },
   "outputs": [],
   "source": [
    "num_tokens = len(vocab.tokens2indices()) - 1\n",
    "num_mel_bins = 64\n",
    "hidden_size= 512\n",
    "num_layers = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "several-impact",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-24T07:58:03.573180Z",
     "start_time": "2021-03-24T07:57:55.241285Z"
    }
   },
   "outputs": [],
   "source": [
    "num_epochs = 7\n",
    "model_dir = 'models/6_recovered'\n",
    "log_every_n_batch = 10\n",
    "\n",
    "model = Model(\n",
    "    num_mel_bins=num_mel_bins,\n",
    "    hidden_size=hidden_size,\n",
    "    num_layers=num_layers,\n",
    "    num_tokens=num_tokens\n",
    ")\n",
    "# load_from_ckpt(model, '/home/e.chuykova/data/ckpt.pt')\n",
    "load_from_ckpt(model, '/home/mnakhodnov/sirius-stt/models/6/epoch_6.pt')\n",
    "model = model.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "difficult-metabolism",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-24T07:58:05.325203Z",
     "start_time": "2021-03-24T07:58:05.300393Z"
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = 2e-4\n",
    "opt = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.CTCLoss(blank=blank_index, reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "instructional-field",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-24T07:58:09.889539Z",
     "start_time": "2021-03-24T07:58:09.870865Z"
    }
   },
   "outputs": [],
   "source": [
    "# ls_train_dataloader.sampler.epoch = 0\n",
    "# open_stt_train_dataloader.sampler.epoch = 0\n",
    "# common_voice_train_dataloader.sampler.epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "expanded-exhibit",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-24T07:58:12.329222Z",
     "start_time": "2021-03-24T07:58:12.308712Z"
    }
   },
   "outputs": [],
   "source": [
    "spectrogram_transform = None\n",
    "spectrogram_transform_first_epoch = None\n",
    "\n",
    "# spectrogram_transform = SpectrogramTransform(freq_mask_param=10, time_mask_param=10)\n",
    "# spectrogram_transform_first_epoch = None\n",
    "\n",
    "# spectrogram_transform = SpectrogramTransform(freq_mask_param=10, time_mask_param=10)\n",
    "# spectrogram_transform_first_epoch = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "parliamentary-origin",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-24T08:51:16.127109Z",
     "start_time": "2021-03-24T07:58:32.375180Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4eb908c758b475f9b71d09f3c153ef7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "359194c00e484586a64acbd5cf0c1d64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c88be6cbc164cbdad2983249e82758e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1982a1a2c9094997ba2f7e8358565da3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4731977a356346d1855784369c95c8fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 of 100 took 2926.453604221344s, train loss: 0.2936456932398035, val loss: 0.2168652572631836, train wer: 0.23655850642945314, val wer: 0.33011100677100674\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "725ab27a42c9429d81a2efd016529fe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-3627c43f5489>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mbeam_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfast_beam_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mspectrogram_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspectrogram_transform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mspectrogram_transform_first_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspectrogram_transform_first_epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m )\n",
      "\u001b[0;32m/home/mnakhodnov/sirius-stt/src/optimization.py\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(model, optimizer, loss_fn, num_epochs, train_dataloader, val_dataloaders, log_every_n_batch, model_dir, vocab, beam_kwargs, spectrogram_transform, spectrogram_transform_first_epoch, scheduler)\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"audios\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"audio_lens\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tokens\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"texts\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text_lens\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                 \u001b[0mdecoder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgreedy_decoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspectrogram_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0musing_spectrogram_transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m             )\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mnakhodnov/sirius-stt/src/optimization.py\u001b[0m in \u001b[0;36mget_model_results\u001b[0;34m(model, audios, audio_lens, tokens, texts, text_lens, vocab, loss_fn, decoder, decoder_kwargs, spectrogram_transform)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mwer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_wer_for_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mnakhodnov/sirius-stt/src/optimization.py\u001b[0m in \u001b[0;36mget_prediction\u001b[0;34m(logprobs, logprobs_lens, vocab, decoder, decoder_kwargs)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mdecoder_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogprobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogprobs_lens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogprobs_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdecoder_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhypos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mkey_value\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mkey_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mhypos\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mnakhodnov/sirius-stt/src/decoding.py\u001b[0m in \u001b[0;36mgreedy_decoder\u001b[0;34m(logprobs, logprobs_lens, vocab, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogprobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0malligmnet_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlookup_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlogprobs_lens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0malligmnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malligmnet_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mhypo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malligmnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mnakhodnov/sirius-stt/vocabulary.py\u001b[0m in \u001b[0;36mlookup_tokens\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m     73\u001b[0m         r\"\"\"\n\u001b[1;32m     74\u001b[0m         \u001b[0mArgs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0mindices\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mused\u001b[0m \u001b[0mto\u001b[0m \u001b[0mlookup\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mcorresponding\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mReturns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "\n",
    "training(\n",
    "    model=model, optimizer=opt, loss_fn=loss_fn, num_epochs=num_epochs, \n",
    "#     train_dataloader=[common_voice_train_dataloader, 'common_voice/train'],\n",
    "#     train_dataloader=[ls_train_dataloader, 'libre_speech/train'],\n",
    "#     train_dataloader=[open_stt_train_dataloader, 'open_stt/train'],\n",
    "    train_dataloader=[combined_dataloader, 'combined/train'],\n",
    "    val_dataloaders={\n",
    "        'open_stt/test': ls_test_dataloader,\n",
    "        'libre_speech/dev': ls_dev_dataloader,\n",
    "        'libre_speech/test': ls_test_dataloader,\n",
    "        'common_voice/val': common_voice_val_dataloader,\n",
    "    }, log_every_n_batch=log_every_n_batch, model_dir=model_dir, vocab=vocab,\n",
    "    beam_kwargs=fast_beam_kwargs, \n",
    "    spectrogram_transform=spectrogram_transform, \n",
    "    spectrogram_transform_first_epoch=spectrogram_transform_first_epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driving-miniature",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
